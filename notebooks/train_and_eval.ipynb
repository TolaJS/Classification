{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421e9caa0fbeda1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-23T22:09:07.160833Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import args\n",
    "import logging\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from model import Classifier\n",
    "from uuid import uuid4\n",
    "from data import ProjectDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.utils import AverageMeter\n",
    "from utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cda0a0e0ed339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('Log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28433adaff6d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimiser, scaler, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    losses = AverageMeter()  # Track the average loss\n",
    "    metrics_tracker = MetricTracker(args.num_classes).to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    counter = 0\n",
    "\n",
    "    #   Pass images and labels to correct device\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        counter += 1\n",
    "\n",
    "        #   Zero the parameter gradient\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        #   Forward pass\n",
    "        with torch.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        #   Backward pass and optimiser step\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimiser)\n",
    "        scaler.update()\n",
    "\n",
    "        #   Metrics Update\n",
    "        losses.update(loss.item(), images.size(0))  # Update loss\n",
    "        metrics_tracker.update(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.data).sum().item()\n",
    "        \n",
    "        metrics = metrics_tracker.compute()\n",
    "        log = format_log_message(mode='Train', i=counter, epoch=epoch, loss=losses.avg, acc=100 * (correct / total), top1= metrics['top1_acc'], top5= metrics['top5_acc'], f1=metrics['f1'])\n",
    "        logger.info(log)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_accuracy = 100 * (correct / total)\n",
    "    metrics = metrics_tracker.compute()\n",
    "    metrics_tracker.reset()\n",
    "    return losses.avg, train_accuracy, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823b0dacfe37f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device, eval_mode=True):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    metrics_tracker = MetricTracker(args.num_classes).to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            with torch.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            metrics_tracker.update(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.data).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * (correct / total)\n",
    "    metrics = metrics_tracker.compute()\n",
    "    metrics_tracker.reset()\n",
    "    if eval_mode:\n",
    "        return losses.avg, test_accuracy\n",
    "    return test_accuracy, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582dbd883f65927",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ProjectDataset(mode='train', root_dir='dataset', seed=args.seed)\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "train_len = len(train_loader.dataset)\n",
    "\n",
    "val_data = ProjectDataset(mode='val', root_dir='dataset', seed=args.seed)\n",
    "val_loader = DataLoader(val_data, batch_size=args.batch_size, shuffle=False)\n",
    "val_len = len(val_loader.dataset)\n",
    "\n",
    "test_data = ProjectDataset(mode='test', root_dir='dataset', seed=args.seed)\n",
    "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "test_len = len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61097de4930faff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = Classifier(num_classes=args.num_classes)\n",
    "model.to(device)\n",
    "model_name = model.model.__class__.__name__\n",
    "\n",
    "if not os.path.exists(\"./checkpoint/\"):\n",
    "    os.mkdir(\"./checkpoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24046d6b68a0d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = [  # Parameters in the model except FC layer\n",
    "        param for name, param in model.named_parameters() if \"fc\" not in str(name)\n",
    "    ]\n",
    "\n",
    "optimiser = torch.optim.Adam(\n",
    "    params=[\n",
    "        {\"params\": base_params},\n",
    "        {\"params\": model.model.fc.parameters(), \"lr\": 0.01},\n",
    "    ],\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "#   SCHEDULER\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimiser, gamma=args.gamma)\n",
    "#   LOSS\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#   SCALER\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "log_dir = (f\"logs/runs/\"\n",
    "           f\"{model.__class__.__name__}/\"\n",
    "           f\"lr_{args.lr}_bs_{args.batch_size}/\"\n",
    "           f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_\"\n",
    "           f\"{uuid4().hex[:6]}\")\n",
    "writer = SummaryWriter(log_dir)\n",
    "handler = logging.FileHandler(f'{log_dir}/log.txt')\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39416787cc1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary(logger, model_name, train_len, val_len, test_len)\n",
    "logger.info(\"=> Start Training\")\n",
    "for epoch in range(args.epochs):\n",
    "    train_loss, train_acc, train_metrics = train(train_loader, model, criterion, optimiser,\n",
    "                                                 scaler, scheduler, device, epoch + 1)\n",
    "    val_loss, val_acc = test(val_loader, model, criterion, device)\n",
    "\n",
    "    writer.add_scalar('Train/Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Train/Accuracy', train_acc, epoch)\n",
    "    writer.add_scalar('Train/Top-1 Accuracy', train_metrics['top1_acc'], epoch)\n",
    "    writer.add_scalar('Train/Top-5 Accuracy', train_metrics['top5_acc'], epoch)\n",
    "    writer.add_scalar('Train/F1-Score', train_metrics['f1'], epoch)\n",
    "    writer.add_scalar('Val/Loss', val_loss, epoch)\n",
    "    writer.add_scalar('Val/Accuracy', val_acc, epoch)\n",
    "\n",
    "    filename = f\"Model-{model_name}-lr_{args.lr}_bs_{args.batch_size}-E{epoch + 1}.pth\"\n",
    "    save_path = os.path.join(args.save_path, filename)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "logger.info(\"=> Training Finished\")\n",
    "handler.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
